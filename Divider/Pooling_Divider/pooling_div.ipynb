{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved out_files/jan_31_pooling_part_1.csv\n",
      "Saved out_files/jan_31_pooling_part_2.csv\n",
      "Saved out_files/jan_31_pooling_part_3.csv\n",
      "Saved out_files/jan_31_pooling_part_4.csv\n",
      "Saved out_files/jan_31_pooling_part_5.csv\n",
      "Saved out_files/jan_31_pooling_part_6.csv\n",
      "Saved out_files/jan_31_pooling_part_7.csv\n",
      "Saved out_files/jan_31_pooling_part_8.csv\n",
      "Saved out_files/jan_31_pooling_part_9.csv\n",
      "Saved out_files/jan_31_pooling_part_10.csv\n",
      "Saved out_files/jan_31_pooling_part_11.csv\n",
      "Saved out_files/jan_31_pooling_part_12.csv\n",
      "Saved out_files/jan_31_pooling_part_13.csv\n",
      "Saved out_files/jan_31_pooling_part_14.csv\n",
      "Saved out_files/jan_31_pooling_part_15.csv\n",
      "Saved out_files/jan_31_pooling_part_16.csv\n",
      "Saved out_files/jan_31_pooling_part_17.csv\n",
      "Saved out_files/jan_31_pooling_part_18.csv\n",
      "Saved out_files/jan_31_pooling_part_19.csv\n",
      "Saved out_files/jan_31_pooling_part_20.csv\n",
      "Saved out_files/jan_31_pooling_part_21.csv\n",
      "Saved out_files/jan_31_pooling_part_22.csv\n",
      "Saved out_files/jan_31_pooling_part_23.csv\n",
      "Saved out_files/jan_31_pooling_part_24.csv\n",
      "Saved out_files/jan_31_pooling_part_25.csv\n",
      "Saved out_files/jan_31_pooling_part_26.csv\n",
      "Saved out_files/jan_31_pooling_part_27.csv\n",
      "Saved out_files/jan_31_pooling_part_28.csv\n",
      "Saved out_files/jan_31_pooling_part_29.csv\n",
      "Saved out_files/jan_31_pooling_part_30.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = '../../CareemData/anon_pooling_jan_24_amman.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Filter the data for the 31st of January 2024\n",
    "filtered_data = data[data['day'] == '1/31/2024']\n",
    "\n",
    "# Divide the filtered data into 30 smaller CSV files\n",
    "num_files = 30\n",
    "rows_per_file = len(filtered_data) // num_files\n",
    "\n",
    "# Handling the case where the number of rows isn't perfectly divisible\n",
    "if len(filtered_data) % num_files != 0:\n",
    "    rows_per_file += 1\n",
    "\n",
    "for i in range(num_files):\n",
    "    start_row = i * rows_per_file\n",
    "    end_row = start_row + rows_per_file\n",
    "    chunk = filtered_data[start_row:end_row]\n",
    "    chunk_file_path = f'out_files/jan_31_pooling_part_{i+1}.csv'\n",
    "    chunk.to_csv(chunk_file_path, index=False)\n",
    "    print(f'Saved {chunk_file_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved out_files\\anon_pooling_part_1.csv\n",
      "Saved out_files\\anon_pooling_part_2.csv\n",
      "Saved out_files\\anon_pooling_part_3.csv\n",
      "Saved out_files\\anon_pooling_part_4.csv\n",
      "Saved out_files\\anon_pooling_part_5.csv\n",
      "Saved out_files\\anon_pooling_part_6.csv\n",
      "Saved out_files\\anon_pooling_part_7.csv\n",
      "Saved out_files\\anon_pooling_part_8.csv\n",
      "Saved out_files\\anon_pooling_part_9.csv\n",
      "Saved out_files\\anon_pooling_part_10.csv\n",
      "Saved out_files\\anon_pooling_part_11.csv\n",
      "Saved out_files\\anon_pooling_part_12.csv\n",
      "Saved out_files\\anon_pooling_part_13.csv\n",
      "Saved out_files\\anon_pooling_part_14.csv\n",
      "Saved out_files\\anon_pooling_part_15.csv\n",
      "Saved out_files\\anon_pooling_part_16.csv\n",
      "Saved out_files\\anon_pooling_part_17.csv\n",
      "Saved out_files\\anon_pooling_part_18.csv\n",
      "Saved out_files\\anon_pooling_part_19.csv\n",
      "Saved out_files\\anon_pooling_part_20.csv\n",
      "Saved out_files\\anon_pooling_part_21.csv\n",
      "Saved out_files\\anon_pooling_part_22.csv\n",
      "Saved out_files\\anon_pooling_part_23.csv\n",
      "Saved out_files\\anon_pooling_part_24.csv\n",
      "Saved out_files\\anon_pooling_part_25.csv\n",
      "Saved out_files\\anon_pooling_part_26.csv\n",
      "Saved out_files\\anon_pooling_part_27.csv\n",
      "Saved out_files\\anon_pooling_part_28.csv\n",
      "Saved out_files\\anon_pooling_part_29.csv\n",
      "Saved out_files\\anon_pooling_part_30.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = '../../CareemData/anon_pooling_jan_24_amman.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Create output directory if it does not exist\n",
    "output_dir = 'out_files'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Divide the data into 30 smaller CSV files\n",
    "num_files = 30\n",
    "rows_per_file = len(data) // num_files\n",
    "\n",
    "# Handling the case where the number of rows isn't perfectly divisible\n",
    "if len(data) % num_files != 0:\n",
    "    rows_per_file += 1\n",
    "\n",
    "for i in range(num_files):\n",
    "    start_row = i * rows_per_file\n",
    "    end_row = start_row + rows_per_file\n",
    "    chunk = data[start_row:end_row]\n",
    "    chunk_file_path = os.path.join(output_dir, f'pooling_jan_31_part_{i+1}.csv')\n",
    "    chunk.to_csv(chunk_file_path, index=False)\n",
    "    print(f'Saved {chunk_file_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in 'day' column before conversion: ['2024-01-14' '2024-01-27' '2024-01-15' '2024-01-24' '2024-01-23'\n",
      " '2024-01-19' '2024-01-25' '2024-01-26' '2024-01-09' '2024-01-29'\n",
      " '2024-01-02' '2024-01-03' '2024-01-28' '2024-01-18' '2024-01-21'\n",
      " '2024-01-01' '2024-01-13' '2024-01-20' '2024-01-22' '2024-01-30'\n",
      " '2024-01-11' '2024-01-10' '2024-01-17' '2024-01-08' '2024-01-04'\n",
      " '2024-01-05' '2024-01-06' '2024-01-16' '2024-01-07' '2024-01-12'\n",
      " '2024-01-31']\n",
      "Unique values in 'day' column after conversion: <DatetimeArray>\n",
      "['2024-01-14 00:00:00', '2024-01-27 00:00:00', '2024-01-15 00:00:00',\n",
      " '2024-01-24 00:00:00', '2024-01-23 00:00:00', '2024-01-19 00:00:00',\n",
      " '2024-01-25 00:00:00', '2024-01-26 00:00:00', '2024-01-09 00:00:00',\n",
      " '2024-01-29 00:00:00', '2024-01-02 00:00:00', '2024-01-03 00:00:00',\n",
      " '2024-01-28 00:00:00', '2024-01-18 00:00:00', '2024-01-21 00:00:00',\n",
      " '2024-01-01 00:00:00', '2024-01-13 00:00:00', '2024-01-20 00:00:00',\n",
      " '2024-01-22 00:00:00', '2024-01-30 00:00:00', '2024-01-11 00:00:00',\n",
      " '2024-01-10 00:00:00', '2024-01-17 00:00:00', '2024-01-08 00:00:00',\n",
      " '2024-01-04 00:00:00', '2024-01-05 00:00:00', '2024-01-06 00:00:00',\n",
      " '2024-01-16 00:00:00', '2024-01-07 00:00:00', '2024-01-12 00:00:00',\n",
      " '2024-01-31 00:00:00']\n",
      "Length: 31, dtype: datetime64[ns]\n",
      "Saved out_files\\anon_pooling_part_1.csv\n",
      "Saved out_files\\anon_pooling_part_2.csv\n",
      "Saved out_files\\anon_pooling_part_3.csv\n",
      "Saved out_files\\anon_pooling_part_4.csv\n",
      "Saved out_files\\anon_pooling_part_5.csv\n",
      "Saved out_files\\anon_pooling_part_6.csv\n",
      "Saved out_files\\anon_pooling_part_7.csv\n",
      "Saved out_files\\anon_pooling_part_8.csv\n",
      "Saved out_files\\anon_pooling_part_9.csv\n",
      "Saved out_files\\anon_pooling_part_10.csv\n",
      "Saved out_files\\anon_pooling_part_11.csv\n",
      "Saved out_files\\anon_pooling_part_12.csv\n",
      "Saved out_files\\anon_pooling_part_13.csv\n",
      "Saved out_files\\anon_pooling_part_14.csv\n",
      "Saved out_files\\anon_pooling_part_15.csv\n",
      "Saved out_files\\anon_pooling_part_16.csv\n",
      "Saved out_files\\anon_pooling_part_17.csv\n",
      "Saved out_files\\anon_pooling_part_18.csv\n",
      "Saved out_files\\anon_pooling_part_19.csv\n",
      "Saved out_files\\anon_pooling_part_20.csv\n",
      "Saved out_files\\anon_pooling_part_21.csv\n",
      "Saved out_files\\anon_pooling_part_22.csv\n",
      "Saved out_files\\anon_pooling_part_23.csv\n",
      "Saved out_files\\anon_pooling_part_24.csv\n",
      "Saved out_files\\anon_pooling_part_25.csv\n",
      "Saved out_files\\anon_pooling_part_26.csv\n",
      "Saved out_files\\anon_pooling_part_27.csv\n",
      "Saved out_files\\anon_pooling_part_28.csv\n",
      "Saved out_files\\anon_pooling_part_29.csv\n",
      "Saved out_files\\anon_pooling_part_30.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = '../../CareemData/anon_pooling_jan_24_amman.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Inspect the unique values in the 'day' column to identify the date format\n",
    "print(\"Unique values in 'day' column before conversion:\", data['day'].unique())\n",
    "\n",
    "# Convert the 'day' column to a datetime object, handling multiple date formats\n",
    "data['day'] = pd.to_datetime(data['day'].str.strip(), errors='coerce')\n",
    "\n",
    "# Inspect the unique values after conversion\n",
    "print(\"Unique values in 'day' column after conversion:\", data['day'].unique())\n",
    "\n",
    "# Define the specific date to filter\n",
    "filter_date = pd.to_datetime('2024-01-31')\n",
    "\n",
    "# Filter the data for the specific date\n",
    "filtered_data = data[data['day'] == filter_date]\n",
    "\n",
    "# Check if any rows were filtered\n",
    "if filtered_data.empty:\n",
    "    print(\"No data found for the date '2024-01-31'.\")\n",
    "else:\n",
    "    # Create output directory if it does not exist\n",
    "    output_dir = 'out_files'\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # Divide the filtered data into 30 smaller CSV files\n",
    "    num_files = 30\n",
    "    rows_per_file = len(filtered_data) // num_files\n",
    "\n",
    "    # Handling the case where the number of rows isn't perfectly divisible\n",
    "    if len(filtered_data) % num_files != 0:\n",
    "        rows_per_file += 1\n",
    "\n",
    "    for i in range(num_files):\n",
    "        start_row = i * rows_per_file\n",
    "        end_row = start_row + rows_per_file\n",
    "        chunk = filtered_data[start_row:end_row]\n",
    "        chunk_file_path = os.path.join(output_dir, f'anon_pooling_part_{i+1}.csv')\n",
    "        chunk.to_csv(chunk_file_path, index=False)\n",
    "        print(f'Saved {chunk_file_path}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
